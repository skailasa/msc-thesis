The performance impact of using \gls{JIT} compilation with Numba on top of Numpy
is illustrated in figure (\ref{fig:3_1_numba}). Here we benchmark the performance
of an implementation of insertion sort, an algorithm with an asymptotic complexity
bound of $O(N^2)$, when using an implementation built with Numpy containers and
compare this to an implementation using JIT compilation on top of Numpy containers.
Both functions are run five times on different inputs of the same size
in figure (\ref{fig:3_1_numba}). On larger problems it becomes apparent
that Numba offers an approximately constant speedup over Numpy, in fact for
sorting arrays of $10^4$ integers the JIT compiled function is $1050 \pm 80$
times faster than the pure Numpy function, where the error is provided to (1 s.f)
\footnote{Please refer to Appendix \ref{app:errors}, Section \ref{sec:app_numba},
for the specifics of the error propagation calculation in this experiment.}.

\begin{figure}[!h]
    \centering

  {\includegraphics[width=0.9\textwidth]{chapter3/numba_speedup.png}}
  \vspace{0pt}
    \caption{The $y$ axis is the multiplicative speedup offered by using Numba in
    conjunction with Numpy for the above benchmarking experiment. The $x$ axis is
    the length of a list to be sorted $N$, plotted with a logarithmic scale. Error
    bars are plotted from the standard deviation in speedup over all trials.}
    \label{fig:3_1_numba}
\end{figure}

To benchmark the efficacy of HDF5 in comparison to serialisation for saving,
and loading it from disk we create an artificial Numpy array composed of
20,000 columns and 10,000 rows of 64 bit floats. This array as a minimum size
of 1.5 GB (1 s.f.), excluding the metadata associated with the type.
This array is saved and loaded 5 times from disk

- Estimate cost savings from each parallelization steps
    - multiproc

- Compare experimental results for FMM vs Direct computation as a function of number of particles for the LAPLACE KERNEL!!!!. Justify usage of laplace kernel (paradigm, easy etc.)

- [DIAGRAM] Need to plot the computational complexity.

- Understand cost of computation via pyexafmm compared to state of the art exafmm-t.

- Comment on where the slowness comes from

- no parallelism for operator evaluations

- no parallelism for memory sharing

- lots of copying of same data.

- subtoptimal data formats for m2l - lots of loading.

- suboptimal size of data structures lead to loading from non-cpu memory - which is slow - could be optimized via numexpr etc in the future or more intricate chunking of memory.

- These are justifiable from the time constraints on full-time development.

Figures required:

- KEY RESULT: Benchmarking figure as a function of N-particles
