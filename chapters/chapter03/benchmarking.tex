All experiments are computed on a 2018 MacBook Pro, with a 2.4 GHz Quad-Core
Intel Core i5 processor and 8GB of memory. Furthermore, data is stored to a
500 GB Apple's proprietary solid state drive.

The performance impact of using \gls{JIT} compilation with Numba on top of Numpy
is illustrated in figure (\ref{fig:3_1_numba}). Here we benchmark the performance
of an implementation of insertion sort, an algorithm with an asymptotic complexity
bound of $O(N^2)$, when using an implementation built with Numpy containers and
compare this to an implementation using JIT compilation on top of Numpy containers.
Both functions are run five times on different inputs of the same size
in figure (\ref{fig:3_1_numba}). On larger problems it becomes apparent
that Numba offers an approximately constant speedup over Numpy, in fact for
sorting arrays of $10^4$ integers the JIT compiled function is $1050 \pm 80$
times faster than the pure Numpy function, where the error is provided to (1 s.f)
\footnote{Please refer to Appendix \ref{app:errors}, Section \ref{sec:app_numba},
for the specifics of the error propagation calculation in this experiment.}.

\begin{figure}[ht]
    \centering

  {\includegraphics[width=0.9\textwidth]{chapter3/numba_speedup.png}}
  \vspace{0pt}
    \caption{The $y$ axis is the multiplicative speedup offered by using Numba in
    conjunction with Numpy for the above benchmarking experiment. The $x$ axis is
    the length of a list to be sorted $N$, plotted with a logarithmic scale. Error
    bars are plotted from the standard deviation in speedup over all trials.}
    \label{fig:3_1_numba}
\end{figure}

To benchmark the efficacy of HDF5 in comparison to serialisation for saving,
and loading data from disk we create a Numpy array composed of
20,000 columns and 10,000 rows of 64 bit floats. This array has a minimum size
of 1.5 GB (1 s.f.), excluding the metadata associated with the type, and is
saved and loaded 5 times from disk for statistics. Serialisation throughout
\gls{PyExaFMM} is computed using the core Python Pickle library. For saving,
serialisation takes $2.7 \pm 0.7$ seconds (1 s.f), and hdf5 takes $1.1 Â± 0.2$ seconds (1 s.f).
For loading from memory, as HDF5's mechanism of returning a database style interface
rather than loading everything to disk, leads to the appearance of rapid
load times, we also measure the time taken to load the entire file to disk from
HDF5. Loading the serialised file takes $3.2 \pm 0.4$ seconds (1 s.f), loading
the HDF5 file to disk takes $1.4 \pm 0.1$ seconds (1 s.f).


- Compare experimental results for FMM vs Direct computation as a function of number of particles for the LAPLACE KERNEL!!!!. Justify usage of laplace kernel (paradigm, easy etc.)

- [DIAGRAM] Need to plot the computational complexity.

- Understand cost of computation via pyexafmm compared to state of the art exafmm-t.

- Comment on where the slowness comes from

- no parallelism for operator evaluations

- no parallelism for memory sharing

- lots of copying of same data.

- subtoptimal data formats for m2l - lots of loading.

- suboptimal size of data structures lead to loading from non-cpu memory - which is slow - could be optimized via numexpr etc in the future or more intricate chunking of memory.

- These are justifiable from the time constraints on full-time development.

Figures required:

- KEY RESULT: Benchmarking figure as a function of N-particles
