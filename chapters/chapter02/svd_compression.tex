As mentioned in Section \ref{sec:2_3_operator_caching}, \gls{PyExaFMM} compresses
a concatenated version of the \gls{M2L} operator matrices using an \gls{SVD}.
The utility of using an SVD, in comparison to say an eigenvalue decomposition, lies
in the fact that it is defined on all matrices, real and complex
\cite{Trefethen:1997:SIAM}. From equation (\ref{eq:2_3_concatenated_m2l}), the
concatenated M2L operators can be written as,

\begin{equation}
    A = \left [ A_1 | A_2 | ... | A_I \right],
    \label{eq:2_4_concatenated_m2l}
\end{equation}

where, as before, $\{A_i | i \in [1, 2, ..., I]\}$ are the M2L operator matrices
for a given source and target box pair and $I$ is the size of a given target box's
interaction list, can be written as a single matrix $A$, where generally
$A \in \mathbb{C}^{T, I \cdot S}$. Here $T$ refers to the number of quadrature
points on a given target box's surface, and $S$ refers to the number of quadrature
points on all the source boxes in its interaction list's surfaces. For non-square
matrices, it is always the case that either the rows or columns (whichever is
greater in number), are linearly dependent. Therefore,

\begin{equation}
    \text{rank}(A) \leq \min (T, I \cdot S),
\end{equation}

using this fact, it's therefore possible to write a lower-rank approximation of $A$.
However, an even lower rank approximation is possible using an SVD,

\begin{equation}
    A = U \Sigma V^*,
\end{equation}

this can also be rephrased as a weighted sum of rank one matrices, or put simply,
written in terms of the products of the left and right singular vectors \cite{Trefethen:1997:SIAM},

\begin{equation}
    A = \sum_{j=1}^{r}\sigma_j u_j v_j^*,
\end{equation}

where $r = \text{rank} (A)$, $\{\sigma_j | j \in [1, ..., r] \}$ are the singular
values and $\{u_j | j \in [1, ..., r] \} $ and $\{v_j | j \in [1, ..., r] \}$
are the left and right singular vectors respectively.

However, noting that singular values are generally arranged in weakly increasing
order in terms of magnitude such that,

\begin{equation}
    \sigma_1 \geq \sigma_2 \geq ... \geq \sigma_{r-1} \geq \sigma_r,
\end{equation}

one can say that the $\tau^{th}$ partial sum where
$\tau \leq r$ captures as much `energy' of $A$ as possible,

\begin{equation}
    A_\tau = \sum_{j=1}^{\tau}\sigma_j u_j v_j^*,
    \label{eq:2_4_sum_svd}
\end{equation}

here the `energy' of an operator is defined in terms of either a 2-norm or the
Frobenius norm as developed in \cite{Trefethen:1997:SIAM}. In fact it can be shown
that for any $\tau$ with $0 \leq \tau \leq r$, if $\tau = \min \{T, I \cdot S\}$
and we define $\sigma_{\tau + 1} = 0$ then,

\begin{equation}
    ||A - A_\tau ||_2 = \sigma_{\tau+1} = 0,
\end{equation}

meaning that $A_\tau$ is the best approximation of $A$, by a matrix of a lower rank
\cite{Trefethen:1997:SIAM}. However, one can also cut off the sum (\ref{eq:2_4_sum_svd}),
if the energy of the approximated $A_\tau$ is within some acceptable tolerance,

\begin{equation}
    ||A - A_\tau ||_2  \leq  \text{tol},
    \label{eq:2_4_svd_tol}
\end{equation}

such that if the sum is cut off at the $k^{th}$ value, leaving the approximation with
$\text{rank}(A_\tau) = k$, where $\sigma_k > \text{tol}$ and
$\sigma_{k+1} \leq \text{tol}$. The \gls{SVD} of the approximation $A_\tau$ can
then be written as,

\begin{equation}
    A_\tau = U_k \Sigma_k V_k^*,
    \label{eq:2_4_decomposed_approximation_a_tau}
\end{equation}

where $U_k$ and $V_k$ have $k$ rows and columns respectively. As $k < r$, this
method is known as a low-rank SVD approximation. The application of
(\ref{eq:2_4_decomposed_approximation_a_tau}) represents a complexity saving in
comparison to applying $A$ directly. This can be seen by considering the full
SVD of a generic matrix $B \in \mathbb{C}^{m, n}$,

\begin{equation}
\begin{pNiceMatrix}[first-row,last-row,first-col,last-col]
 &    &    &   \leftarrow n \rightarrow  &     & \\
\uparrow &    &    &   &    &   & \\
m &    &    & B &    &  &  \\
\downarrow &    &    &   &    &  &  \\
 &    &    &   &    &      \\
\end{pNiceMatrix} = \begin{pNiceMatrix}[first-row, last-row, first-col, last-col]
&  &  \leftarrow m \rightarrow  &   &  \\
\uparrow &  &  &   & \\
m &  & U  &   & \\
\downarrow &  &   &   & \\
&  &   &   & \\
\end{pNiceMatrix} \begin{pNiceMatrix}[first-row, last-row, first-col, last-col]
&  &  \leftarrow n \rightarrow  &   &  \\
\uparrow &  &  &   & \\
m &  & \Sigma  &   & \\
\downarrow &  &   &   & \\
&  &   &   & \\
\end{pNiceMatrix}\begin{pNiceMatrix}[first-row, last-row, first-col, last-col]
&  &  \leftarrow n \rightarrow  &   &  \\
\uparrow &  &  &   & \\
n &  & V^*  &   & \\
\downarrow &  &   &   & \\
&  &   &   & \\
\end{pNiceMatrix}
\label{eq:2_4_b_svd}
\end{equation}

where the dimension of each matrix in the \gls{SVD} decomposition has been
illustrated. For an approximated matrix $B_\tau$, with $\text{rank}(B_\tau) = k$,
we find by applying (\ref{eq:2_4_sum_svd}),

\begin{equation}
    \begin{pNiceMatrix}[first-row,last-row,first-col,last-col]
        &    &    &   \leftarrow n \rightarrow  &     & \\
       \uparrow &    &    &   &    &   & \\
       m &    &    & B_\tau &    &  &  \\
       \downarrow &    &    &   &    &  &  \\
        &    &    &   &    &      \\
       \end{pNiceMatrix} = \begin{pNiceMatrix}[first-row, last-row, first-col, last-col]
       &  &  \leftarrow k \rightarrow  &   &  \\
       \uparrow &  &  &   & \\
       m &  & U  &   & \\
       \downarrow &  &   &   & \\
       &  &   &   & \\
       \end{pNiceMatrix} \begin{pNiceMatrix}[first-row, last-row, first-col, last-col]
       &  &  \leftarrow k \rightarrow  &   &  \\
       \uparrow &  &  &   & \\
       k &  & \Sigma  &   & \\
       \downarrow &  &   &   & \\
       &  &   &   & \\
       \end{pNiceMatrix}\begin{pNiceMatrix}[first-row, last-row, first-col, last-col]
       &  &  \leftarrow n \rightarrow  &   &  \\
       \uparrow &  &  &   & \\
       k &  & V^*  &   & \\
       \downarrow &  &   &   & \\
       &  &   &   & \\
       \end{pNiceMatrix}
\label{eq:2_4_b_tau_svd}
\end{equation}

If we were to apply the \gls{SVD} decomposition of $B$ to a given vector
$x \in \mathbb{C}^{n}$, this would result in an asymptotic complexity of
$O(m^2 + n^2)$ from (\ref{eq:2_4_b_svd}). Compared with the application of the
\gls{SVD} decomposition of $B_\tau$, which results in an asymptotic complexity
of $O(k(m + n))$. As long as $k < \min (m, n)$, this represents a complexity
saving and therefore faster matrix-vector products with the approximation
$B_\tau$. This internal dimension $k$ which represents the amount of compression
is often referred to as the target rank. Optimum choices for $k$ for compressing
the concatenated \gls{M2L} operator matrix, $A$, are explored in
Chapter \ref{chpt:3} Section \ref{sec:3_2_svd_params}.

Currently, \gls{PyExaFMM} computes the \gls{SVD} using the provided function
from the SciPy module, which itself calls an optimised \textbf{\gls{LAPACK}}
function. However, this implementation is optimised to reduce the number
of \textbf{\gls{FLOPS}}, rather than take advantage of advances in modern
\textbf{\gls{heterogenous}} multicore machines, and
distributed memory programming paradigms which are more often limited by the
communication and data transfer overhead between processes, than by number of
operations \cite{Halko:2011:SIAM}. Halko et. al introduce new `randomised'
approaches for generating low-rank approximations of matrices, which can be
optimised for modern heterogenous and distributed computing environments as they
consist of processes which can be easily parallelised \cite{Halko:2011:SIAM}.

The low-rank approximation implemented in \gls{PyExaFMM} via a truncated
\gls{SVD} (\ref{eq:2_4_decomposed_approximation_a_tau}) is costly to compute
for large matrices. This is because a full \gls{SVD} decomposition must be computed, of
which all but the first $k$ terms are ignored.
We roughly derive the randomised method in application to the \gls{SVD} below, however guide
the reader to the literature for further detail \cite{Erichson:2019:JOSS,Halko:2011:SIAM}.
Consider again the matrix $B \in \mathbb{C}^{m, n}$. Randomised methods consist
of two logical steps to find low-rank approximations of $B$: Step one, construct
a low-dimensional subspace that approximates the column space of $B$, i.e.
find an orthonormal matrix $Q \in \mathbb{C}^{m, k}$ such that,

\begin{equation}
    B \approx QQ^*B
    \label{eq:2_4_step_1_randomised}
\end{equation}

where $k$ takes the same meaning as before as the target rank of the compressed
matrix. Step two, form a smaller matrix defined $C := Q^*B \> \in \mathbb{C}^{k, n}$,
by which $B$ is restricted to a lower dimensional space spanned by the basis
$Q$.

Step one is `randomised' by drawing $k$ random vectors $\{ \omega_i | \omega_i \in \mathbb{R}^m \}_{i=1}^k$,
from a distribution, for example the standard normal distribution, and finding the
resulting projections due to $B$, $y_i = B \omega_i$. In matrix form we can write,

\begin{equation}
    Y := B \Omega
\end{equation}

where $\Omega$ is a matrix with columns formed formed from $\omega_i$.
Probability theory guarantees a high-chance of each $y_i$ being linearly independent
\cite{Erichson:2019:JOSS}. This step provides the opportunity to implement
\gls{task-level-parallelism} by distributing the matrix-vector products over multiple
processor cores \cite{Halko:2011:SIAM}. The resultant matrix, $Y$, can be
orthonormalised via a QR decomposition to find,

\begin{equation}
    Y =: QR
\end{equation}

where $Q$ is the orthonormal basis we desire, and $R$ as usual is an upper triangular
matrix. This definition of $Q$ satisfies (\ref{eq:2_4_step_1_randomised}). Step two
is now computed as,

\begin{equation}
    C := Q^* B
    \label{eq:2_4_step_2_randomised}
\end{equation}

which provides the compressed matrix $C \in \mathbb{C}^{k, n}$.
We can now place the \gls{SVD} into the randomised framework above. Consider a compressed matrix
$C \in \mathbb{C}^{k, n}$ obtained via the two steps detailed above. An
ordinary deterministic implementation can be used to calculate the
the \gls{SVD} of $C$ cheaply in comparison to the full SVD of $B$ as long as
$k \ll n$, such that

\begin{equation}
    C = \tilde{U}\Sigma V^*
    \label{eq:2_4_svd_of_c}
\end{equation}

here we cheaply obtain the first $k$ right singular vectors of $B$,
from $V \in \mathbb{C}^{n, k}$, as well as the first $k$ singular values
$\Sigma \in \mathbb{R}^{k, k}$. To find the entire \gls{SVD} of $B$, including the
left singular vectors, we notice that,

\begin{equation}
    B \approx Q Q^* B = QC = Q \tilde{U} \Sigma V^* := U\Sigma V^*
\end{equation}

where we combine (\ref{eq:2_4_step_1_randomised}) and (\ref{eq:2_4_step_2_randomised})
with the results of the \gls{SVD} of $C$ (\ref{eq:2_4_svd_of_c}), and define the
left singular vectors as $U = Q \tilde{U}$. In terms of \gls{PyExaFMM}, randomised
methods for low-rank approximations of the M2L operator matrices are attractive candidates
for future implementation as they represent the state of the art for accelerating the computation of an \gls{SVD}.
As noted in Chapter \ref{chpt:1_introduction} Section \ref{sec:1_2_kifmm_overview},
\gls{PyExaFMM}'s approach differs to other major \gls{KIFMM} implementations \cite{Malhotra:2015:CCP,exafmm}.
The differing approach will therefore make an interesting point of comparison.
For the concatenated \gls{M2L} matrix, the application of the low rank \gls{SVD}
scheme over all source boxes in a given target box's interaction list has
an asymptotic complexity of $O(k(T + I \cdot S))$, as $S$ and $T$ are set to be
equivalent by \gls{PyExaFMM} this may be written as $O(k(p^2 + I \cdot p^2))$, where
$p$ is the order of the multipole and local expansion where we have used
(\ref{eq:2_2_quadrature_points}). This compares favourably with the \gls{FFT} scheme described in Chapter
\ref{chpt:1_introduction} Section \ref{sec:1_2_kifmm_overview} as long as $k \ll p^2$,
which resulted in an asymptotic complexity of $O(I(p^3 \log(p)))$, or as $I$ is
bounded by a constant for three dimensional simulations, $O(p^3\log(p))$. Furthermore,
an open source \textbf{\gls{apache}} \textbf{software foundation} lead software implementation for
a randomised \gls{SVD} based on these methods, designed to take advantage
of \gls{instruction-level-parallelism}, is readily available \cite{mahout}.
